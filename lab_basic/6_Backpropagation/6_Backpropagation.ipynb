{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HaMLeT\n",
    "\n",
    "## Session 6: Backpropagation\n",
    "This lab is designed by Leon Weninger and Raphael Kolk\n",
    "\n",
    "### Goal of this Session\n",
    "\n",
    "In this session you will, step by step, implement a backpropagation algorithm yourself without using any deep learning libraries. You should already be familiar with Python as well as NumPy (a package for scientific computing with Python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given code\n",
    "\n",
    "**Task 0:** Familiarize yourself briefly with the given code. Pay particural attention to the `Layer` and `Cost` classes, from which you will derive the classes you implement, and the `Sigmoid` layer which is predefined as an example. You'll also need to execute the cells in this section once.\n",
    "\n",
    "The following code loads the data and trains the network in a similar fashion as in the last session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_{out} = w^T x_{in} + b$$\n",
    "$$\\delta_{in} = w^T \\delta_{out}$$\n",
    "$$\\frac{\\partial C}{\\partial b} = \\delta_{out}$$\n",
    "$$\\frac{\\partial C}{\\partial w} = x_{in} \\delta_{out}$$\n",
    "\n",
    "$$x_{out} = \\sigma (x_{in})$$\n",
    "$$\\delta_{in} = \\delta_{out} \\odot \\sigma' (x_{in})$$\n",
    "\n",
    "$$cost = \\frac{1}{N} \\sum_{i=1}^N ( x_{in} - target )^2$$\n",
    "$$\\delta_{in} = x_{in} - target $$\n",
    "\n",
    "$$ \\frac{\\partial cost}{\\partial x_{in}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from load_mnist import MNIST\n",
    "\n",
    "\n",
    "def vectorize(j):\n",
    "    label_vector = np.zeros((1, 10))\n",
    "    label_vector[0, int(j)] = 1.0\n",
    "    return label_vector\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    mnist = MNIST()\n",
    "    images, labels = mnist.data, mnist.target\n",
    "\n",
    "    image_size = images.shape[1]\n",
    "    label_size = labels.shape[1]\n",
    "\n",
    "    random_permutation = np.random.permutation(images.shape[0])\n",
    "    images = images[random_permutation, :]\n",
    "    labels = labels[random_permutation, :]\n",
    "    \n",
    "    images = (images - np.mean(images))/np.std(images)\n",
    "\n",
    "    return images, labels, image_size, label_size\n",
    "\n",
    "\n",
    "def train(net, cost_function, number_epochs, batch_size, learning_rate):\n",
    "    images, labels, image_size, label_size = load_data()\n",
    "    training_images, validation_images = images[:50000], images[50000:]\n",
    "    training_labels, validation_labels = labels[:50000], labels[50000:]\n",
    "\n",
    "    for e in range(number_epochs):\n",
    "        cost = train_epoch(e, net, training_images, training_labels, cost_function, batch_size, learning_rate)\n",
    "        accuracy = validate_epoch(e, net, validation_images, validation_labels, batch_size)\n",
    "        print('cost=%5.6f, accuracy=%2.6f' % (cost, accuracy), flush=True)\n",
    "\n",
    "\n",
    "def train_epoch(e, net, images, labels, cost_function, batch_size, learning_rate):\n",
    "    epoch_cost = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(images), batch_size), ascii=False, desc='training,   e=%i' % e):\n",
    "        batch_images = images[i:min(i + batch_size, len(images)), :]\n",
    "        batch_labels = labels[i:min(i + batch_size, len(labels)), :]\n",
    "\n",
    "        # zero the gradients\n",
    "        net.zero_gradients()\n",
    "\n",
    "        # forward pass\n",
    "        prediction = net.forward(batch_images)\n",
    "        cost = cost_function.estimate(batch_labels, prediction)\n",
    "\n",
    "        # backward pass\n",
    "        dprediction = cost_function.gradient(cost)\n",
    "        net.backward(dprediction)\n",
    "\n",
    "        # update the parameters using the computed gradients via stochastic gradient descent.\n",
    "        net.update_parameters(learning_rate)\n",
    "\n",
    "        epoch_cost += np.mean(cost)\n",
    "\n",
    "    return epoch_cost\n",
    "\n",
    "\n",
    "def validate_epoch(e, net, images, labels, batch_size):\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(images), batch_size), ascii=False, desc='validation, e=%i' % e):\n",
    "        batch_images = images[i:min(i + batch_size, len(images)), :]\n",
    "        batch_labels = labels[i:min(i + batch_size, len(labels)), :]\n",
    "\n",
    "        # compute predicted probabilities.\n",
    "        predictions = net.forward(batch_images)\n",
    "\n",
    "        # find the most probable class label.\n",
    "        n_correct += sum(np.argmax(batch_labels, axis=1) == np.argmax(predictions, axis=1))\n",
    "        n_total += batch_labels.shape[0]\n",
    "\n",
    "    return n_correct / n_total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the sigmoid function and its derivative you implemented in the previous session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_function(var):\n",
    "    return 1.0 / (1.0 + np.exp(-var))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid_function(z) * (1 - sigmoid_function(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following abstract classes should serve as parent classes for all the different layers and cost functions which you will implement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        # Initialize all member variables of the layer.       \n",
    "        pass\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # Implemets for forward pass of the layer and returns x_out.\n",
    "        pass\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # Implements the backward pass of the layer and returns d_in.\n",
    "        pass\n",
    "\n",
    "    def zero_gradients(self):\n",
    "        # Sets all gradients of the layer to zero.\n",
    "        pass\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        # Update the parameters of the layer with the help of the gradients stored during the backward pass.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Cost:\n",
    "    def __init__(self):\n",
    "        # Initialize all member variables of the cost function.\n",
    "        pass\n",
    "\n",
    "    def estimate(self, target, prediction):\n",
    "        # Estimates and return the cost with respect to the predicted label and a target label previously set by set_target().\n",
    "        pass\n",
    "\n",
    "    def gradient(self, cost):\n",
    "        # Calculates and returns the gradient with respect to the cost.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class derived from the Layer class implements the forward and backward pass of the sigmoid activation function alread known from the previous session and serves as an example for you. Since it does not have learnable parameters, no `update_parameters` or `zero_gradients` function needs to be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        self.x_in = None\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        self.x_in = x_in\n",
    "        x_out = sigmoid_function(x_in)\n",
    "        return x_out\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        d_in = d_out * sigmoid_derivative(self.x_in)\n",
    "        return d_in\n",
    "    \n",
    "    def zero_gradients(self):\n",
    "        pass\n",
    "    \n",
    "    def update_parameters(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Foundation \n",
    "\n",
    "**Task 1a:** Take a peace of paper and a pencil, use your knowledge from the preparation material and the introduction slides and fill in the gaps in the preparation material. Having written the formulas down, please check with the tutor if they are correct.\n",
    "\n",
    "Before starting to work on the code, think about batched forward- and backward passing. We keep to the convention of having the batch as the first dimension of all tensors. This is consistent with modern Deep Learning Frameworks, as you will get to know in the next session. However, this convention may change when tensors needs to be transposed when performing multiplications and additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Implementation\n",
    "\n",
    "**Task 2a:** Implement the `forward` function for the `Linear` layer. Remember to store the `x_in` for use in the backward pass.\n",
    "\n",
    "**Task 2b:** Implement the `estimate` function for the `MeanSquareError` cost, which estimates the cost after a ground truth target is set. Remember to store the `prediction` for calculating the gradient.\n",
    "\n",
    "**Task 2c:** Implement the `gradient` function for the `MeanSquareError` cost, which calculates the gradient with respect to the cost. Use the `prediction` stored during the forward pass.\n",
    "\n",
    "**Task 2d:** Implement the `backward` function for the `Linear` layer. The function should also calculate and accumulate the gradient of `w` and `b` with regard to the error.\n",
    "\n",
    "**Task 2e:** Implement the `update_parameters` function for the `Linear` layer, i.e., use the gradients `dw` and `db` together with a given `learning_rate` to update the parameters `w` and `b` accordingly.\n",
    "\n",
    "**Task 2f:** Test your implementation by propagating random input through a linear layer followed by a sigmoid layer, estimating the mean square error to a random target, calculating the gradient and propagating it back through the sigmoid and linear layer. Afterwards update the parameters of the linear layer using the function you implemented.\n",
    "\n",
    "**Task 3a:** Implement the `Network` class which can encapsulate multiple layers. It offers the same interface as a layer and is therefore derived from the `Layer` parent as well. Make sure to implement all member functions needed. The `forward` function propagates a given input through all encapsulated layers and returns the final prediction of the network, whereas the `backward` function propagates a given gradient through all layers in reversed order. `zero_gradients` and `update_parameters` invoke the respective functions of the encapsulated layers.\n",
    "\n",
    "**Task 3b:** Test your implementation analogous to task 2f but using the `Network` class to encapsulate the linear and sigmoid layer.\n",
    "\n",
    "**Task 4:** Train the network you just implemented using the dataloader and train function given above and the hyperparameter given below.\n",
    "\n",
    "**Task 5:** Come up with a more sophisticated network structure and adjust the hyperparameter in order to increase the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, n_in, n_out, initial_sigma=0.1):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "\n",
    "        self.w = initial_sigma * np.random.randn(n_out, n_in)\n",
    "        self.b = np.zeros((1, n_out))\n",
    "\n",
    "        self.zero_gradients()\n",
    "\n",
    "        self.x_in = None\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # ----- Add code for task 2a between comments -----\n",
    "        # 'Implement the forward function for the Linear layer. \n",
    "        # Remember to store the x_in for use in the backward pass.'\n",
    "        self.x_in = x_in\n",
    "        \n",
    "        x_out = [np.dot(np.expand_dims(sample,axis=0), np.transpose(self.w)) + self.b for sample in x_in]\n",
    "        x_out = np.array(x_out)\n",
    "        assert self.b.shape == np.dot(np.expand_dims(x_in[0], axis=0), np.transpose(self.w)).shape\n",
    "        # -------------------------------------------------\n",
    "        return np.squeeze(x_out)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # ----- Add code for task 2d between comments -----\n",
    "        # 'Implement the backward function for the Linear layer. The function should also calculate \n",
    "        # and accumulate the gradient of w and b with regard to the error.'\n",
    "\n",
    "        self.dw = np.matmul(np.transpose(d_out), self.x_in)\n",
    "\n",
    "        self.db = d_out.sum(axis=0, keepdims=True) # um es einheitlich zu machen die 600 'l√∂schen' und damit die dimension nicht gel√∂scht wird, das keepdims\n",
    "        assert self.dw.shape == (self.n_out, self.n_in)\n",
    "        assert self.db.shape == (1, self.n_out)\n",
    "\n",
    "        self.d_in = np.matmul(d_out, self.w)\n",
    "        self.dx = self.d_in\n",
    "        assert self.dx.shape == (d_out.shape[0], self.n_in)\n",
    "        # -------------------------------------------------\n",
    "        return self.d_in\n",
    "\n",
    "    def zero_gradients(self):\n",
    "        self.dw = np.zeros((self.n_out, self.n_in))\n",
    "        self.db = np.zeros((1, self.n_out))\n",
    "        self.dx = np.empty((0, self.n_in))\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        # ----- Add code for task 2e between comments -----\n",
    "        # 'Implement the update_parameters function for the Linear layer, \n",
    "        # i.e., use the gradients dw and db together with a given \n",
    "        # learning_rate to update the parameters w and b accordingly.'\n",
    "        self.w = self.w - learning_rate * self.dw\n",
    "        self.b = self.b - learning_rate * self.db\n",
    "        # -------------------------------------------------\n",
    "\n",
    "\n",
    "class MeanSquareError(Cost):\n",
    "    def __init__(self):\n",
    "        self.prediction = None\n",
    "        self.target = None\n",
    "\n",
    "    def estimate(self, target, prediction):\n",
    "        # ----- add code for task 2b between comments -----\n",
    "        # 'Implement the estimate function for the MeanSquareError cost, \n",
    "        # which estimates the cost after a ground truth target is set. \n",
    "        # Remember to store the prediction for calculating the gradient.'\n",
    "        self.target = target\n",
    "        self.prediction = prediction\n",
    "        cost = 0.5 * ((target-prediction)**2).mean(axis=None)\n",
    "        # -------------------------------------------------\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, cost):\n",
    "        # ----- add code for task 2c between comments -----\n",
    "        # 'Implement the gradient function for the MeanSquareError cost, \n",
    "        # which calculates the gradient. Use the \n",
    "        # prediction stored during the forward pass.'\n",
    "        gradient = self.prediction - self.target\n",
    "        # -------------------------------------------------\n",
    "        return gradient\n",
    "\n",
    "\n",
    "class Network(Layer):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    # ----- add code for task 3a between comments -----\n",
    "    # Implement the Network class which can encapsulate multiple layers. It offers the same \n",
    "    # interface as a layer and is therefore derived from the Layer parent as well. Make sure \n",
    "    # to implement all member functions needed. The forward function propagates a given input \n",
    "    # through all encapsulated layers and returns the final prediction of the network, whereas \n",
    "    # the backward function propagates a given gradient through all layers in reversed order. \n",
    "    # zero_gradients and update_parameters invoke the respective functions of the encapsulated layers.\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        buffer = x_in\n",
    "        for layer in self.layers:\n",
    "            buffer = layer.forward(buffer)\n",
    "        x_out = buffer\n",
    "        return x_out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # Implements the backward pass of the layer and returns d_in.\n",
    "        buffer = d_out\n",
    "        for layer in reversed(self.layers): # because of going backwards\n",
    "            buffer = layer.backward(buffer)\n",
    "        d_in = buffer\n",
    "        return d_in\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        # Update the parameters of the layer with the help of the gradients stored during the backward pass.\n",
    "        for layer in self.layers:\n",
    "            layer.update_parameters(learning_rate)\n",
    "    # -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:373: DeprecationWarning: Passing 'n_values' is deprecated in version 0.20 and will be removed in 0.22. You can use the 'categories' keyword instead. 'n_values=n' corresponds to 'categories=[range(n)] * n_features'.\n",
      "  warnings.warn(msg, DeprecationWarning)\n",
      "training,   e=0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:11<00:00, 370.15it/s]\n",
      "validation, e=0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 839.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=126.924547, accuracy=0.853550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:10<00:00, 386.59it/s]\n",
      "validation, e=1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1032.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=45.024823, accuracy=0.908350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:10<00:00, 396.43it/s]\n",
      "validation, e=2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 871.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=30.006822, accuracy=0.924050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:07<00:00, 527.24it/s]\n",
      "validation, e=3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 837.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=24.190776, accuracy=0.934850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:07<00:00, 529.54it/s]\n",
      "validation, e=4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:03<00:00, 528.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=20.496776, accuracy=0.942200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:08<00:00, 465.10it/s]\n",
      "validation, e=5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1001.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=17.780754, accuracy=0.948500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:10<00:00, 408.64it/s]\n",
      "validation, e=6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 868.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=15.658349, accuracy=0.953100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:08<00:00, 518.25it/s]\n",
      "validation, e=7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:02<00:00, 823.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=13.948911, accuracy=0.956800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:08<00:00, 482.63it/s]\n",
      "validation, e=8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 941.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=12.548150, accuracy=0.959700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:11<00:00, 365.92it/s]\n",
      "validation, e=9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:02<00:00, 829.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=11.384239, accuracy=0.962450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:11<00:00, 375.19it/s]\n",
      "validation, e=10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1149.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=10.395047, accuracy=0.964650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:08<00:00, 494.81it/s]\n",
      "validation, e=11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1189.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=9.535519, accuracy=0.966100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:09<00:00, 445.90it/s]\n",
      "validation, e=12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 869.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=8.781530, accuracy=0.967200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:07<00:00, 538.53it/s]\n",
      "validation, e=13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1073.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=8.120255, accuracy=0.968100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:06<00:00, 611.95it/s]\n",
      "validation, e=14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 946.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=7.539291, accuracy=0.968600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:06<00:00, 618.09it/s]\n",
      "validation, e=15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1225.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=7.023863, accuracy=0.969100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:06<00:00, 691.01it/s]\n",
      "validation, e=16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1223.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=6.562749, accuracy=0.970100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:08<00:00, 484.01it/s]\n",
      "validation, e=17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1051.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=6.147625, accuracy=0.970500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:07<00:00, 565.43it/s]\n",
      "validation, e=18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1202.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=5.772353, accuracy=0.971050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:09<00:00, 454.73it/s]\n",
      "validation, e=19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:02<00:00, 603.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=5.432285, accuracy=0.971600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:09<00:00, 429.05it/s]\n",
      "validation, e=20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:02<00:00, 701.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=5.123209, accuracy=0.971750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:07<00:00, 578.46it/s]\n",
      "validation, e=21: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1009.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=4.840558, accuracy=0.972200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:06<00:00, 620.79it/s]\n",
      "validation, e=22: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1121.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=4.580065, accuracy=0.972500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:07<00:00, 553.75it/s]\n",
      "validation, e=23: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1035.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=4.339354, accuracy=0.972800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:09<00:00, 441.71it/s]\n",
      "validation, e=24: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 933.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=4.116921, accuracy=0.972950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:10<00:00, 395.88it/s]\n",
      "validation, e=25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 870.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=3.911132, accuracy=0.973400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:08<00:00, 478.88it/s]\n",
      "validation, e=26: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1020.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=3.720788, accuracy=0.973450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:07<00:00, 557.76it/s]\n",
      "validation, e=27: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:02<00:00, 719.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=3.544855, accuracy=0.973600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:08<00:00, 502.88it/s]\n",
      "validation, e=28: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1085.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=3.381577, accuracy=0.973800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:06<00:00, 674.95it/s]\n",
      "validation, e=29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1199.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=3.229010, accuracy=0.973850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:06<00:00, 657.08it/s]\n",
      "validation, e=30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1039.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=3.086409, accuracy=0.974150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:06<00:00, 659.76it/s]\n",
      "validation, e=31: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:02<00:00, 700.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=2.952945, accuracy=0.974250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:06<00:00, 604.89it/s]\n",
      "validation, e=32: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1080.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=2.827343, accuracy=0.974400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:06<00:00, 617.76it/s]\n",
      "validation, e=33: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:03<00:00, 481.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=2.708508, accuracy=0.974550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:08<00:00, 464.68it/s]\n",
      "validation, e=34: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:02<00:00, 679.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=2.596043, accuracy=0.974600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:06<00:00, 656.33it/s]\n",
      "validation, e=35: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:03<00:00, 533.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=2.490319, accuracy=0.974550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:10<00:00, 381.03it/s]\n",
      "validation, e=36: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:02<00:00, 636.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=2.391937, accuracy=0.974750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:09<00:00, 430.55it/s]\n",
      "validation, e=37: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1233.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=2.300936, accuracy=0.975000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:07<00:00, 524.85it/s]\n",
      "validation, e=38: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1209.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=2.216490, accuracy=0.975250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "training,   e=39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4167/4167 [00:06<00:00, 643.09it/s]\n",
      "validation, e=39: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1667/1667 [00:01<00:00, 1112.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost=2.137583, accuracy=0.975400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "input_size = 28**2\n",
    "label_size = 10\n",
    "batch_size = 600\n",
    "learning_rate = 0.0001\n",
    "number_epochs = 100\n",
    "\n",
    "random_input = np.random.rand(batch_size, input_size)\n",
    "random_label = np.random.rand(batch_size, label_size)\n",
    "\n",
    "linear_layer = Linear(input_size, label_size)\n",
    "sigmoid_layer = Sigmoid()\n",
    "cost_function = MeanSquareError()\n",
    "\n",
    "# ----- add code for task 2f between comments -----\n",
    "# 'Test your implementation by propagating random input through \n",
    "# a linear layer followed by a sigmoid layer, estimating the mean\n",
    "# square error to a random target, calculating the gradient and \n",
    "# propagating it back through the sigmoid and linear layer. \n",
    "# Afterwards update the parameters of the linear layer using the \n",
    "# function you implemented.'\n",
    "\n",
    "#Forward Pass\n",
    "linear_for = linear_layer.forward(random_input)\n",
    "sigmoid_for = sigmoid_layer.forward(linear_for)\n",
    "\n",
    "#Calculate Output error vector\n",
    "mse = cost_function.estimate(random_label, sigmoid_for)\n",
    "cost_gradient = cost_function.gradient(mse)\n",
    "\n",
    "#Backpropagate Error\n",
    "sigmoid_back = sigmoid_layer.backward(cost_gradient)\n",
    "linear_back = linear_layer.backward(sigmoid_back)\n",
    "# -------------------------------------------------\n",
    "\n",
    "\n",
    "# ----- add code for task 3b between comments -----\n",
    "#Test your implementation analogous to task 2f but using the Network class \n",
    "#to encapsulate the linear and sigmoid layer\n",
    "\n",
    "#Create instance of Network Class\n",
    "layers = [linear_layer, sigmoid_layer]\n",
    "net = Network(layers)\n",
    "\n",
    "#Forward Pass\n",
    "net_forward = net.forward(random_input)\n",
    "\n",
    "#Calculate Error\n",
    "cost = cost_function.estimate(random_label, net_forward)\n",
    "grad = cost_function.gradient(cost)\n",
    "\n",
    "#Backpropagate Error\n",
    "net_backward = net.backward(grad)\n",
    "\n",
    "# -------------------------------------------------\n",
    "\n",
    "# ----- add code for task 4 between comments ------\n",
    "# Train the network you just implemented using the dataloader and train \n",
    "# function given above and the hyperparameter given below.\n",
    "\n",
    "#Train the Network\n",
    "#train(net, cost_function, number_epochs, batch_size, learning_rate)\n",
    "\n",
    "# -------------------------------------------------\n",
    "\n",
    "# ----- add code for task 5 between comments ------\n",
    "#'Come up with a more sophisticated network structure and \n",
    "# adjust the hyperparameter in order to increase the accuracy.'\n",
    "\n",
    "#Adjust Hyperparameters\n",
    "batch_size = 12\n",
    "learning_rate = 0.01\n",
    "number_epochs = 40\n",
    "\n",
    "#Train the Network\n",
    "layers_2 = [Linear(n_in=28**2, n_out=128), Sigmoid(), \n",
    "           Linear(n_in=128, n_out=32), Sigmoid(),\n",
    "           Linear(n_in=32, n_out=10), Sigmoid()]\n",
    "\n",
    "net1 = Network(layers_2)\n",
    "train(net1, cost_function, number_epochs, batch_size, learning_rate)\n",
    "\n",
    "\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback\n",
    "\n",
    "Aaaaaand we're done üëèüèºüçª\n",
    "\n",
    "If you have any suggestions on how we could improve this session, please let us know in the following cell. What did you particularly like or dislike? Did you miss any contents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Tasks\n",
    "\n",
    "**Task 6a:** Implement the `forward` function for the `SoftMax` layer.\n",
    "\n",
    "**Task 6b:** Implement the `estimate` function for the `CrossEntropy` cost.\n",
    "\n",
    "**Task 6c:** Implement the `gradient` function for the `CrossEntropy` cost.\n",
    "\n",
    "**Task 6d:** Implement the `backward` function for the `SoftMax` layer.\n",
    "\n",
    "**Task 6e:** Test your implementation by setting up a network using the soft max layer and cross entropy cost in combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(Layer):\n",
    "    def __init__(self):\n",
    "        self.x_out = None\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # ----- add code for task 6a between comments -----\n",
    "        # -------------------------------------------------\n",
    "        return self.x_out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # ----- add code for task 6d between comments -----\n",
    "        # -------------------------------------------------\n",
    "        return d_in\n",
    "\n",
    "\n",
    "class CrossEntropy(Cost):\n",
    "    def __init__(self):\n",
    "        self.x_in = None\n",
    "        self.target = None\n",
    "        self.eps = 1e-12\n",
    "\n",
    "    def estimate(self, target, x_in):\n",
    "        # ----- add code for task 6b between comments -----\n",
    "        # -------------------------------------------------\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, d_out):\n",
    "        # ----- add code for task 6c between comments -----\n",
    "        # -------------------------------------------------\n",
    "        return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "input_size = 28**2\n",
    "label_size = 10\n",
    "batch_size = 600\n",
    "learning_rate = 0.00001\n",
    "number_epochs = 100\n",
    "\n",
    "# ----- add code for task 6e between comments -----\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
